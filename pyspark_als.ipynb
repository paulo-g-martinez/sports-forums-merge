{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql import Row, functions as F \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.mllib.recommendation import ALS,MatrixFactorizationModel, Rating\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_json_lines(bigline):\n",
    "    split_line = bigline[1:-1].split('}{')\n",
    "    all_lines = []\n",
    "    for s in split_line:\n",
    "        all_lines.append('{' + s + '}')\n",
    "    print(\"Returning {} lines\".format(len(all_lines)))\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ReadInData\") \\`\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Reading in kinesis data\")\n",
    "schemaString = (\n",
    "   \"authorKey contentKey pathRoot publishedDate referrer\"\n",
    "   \"siteKey timeStamp userAgent userId userStatus\")\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(field_name, StringType(), True)\n",
    "     for field_name in schemaString.split()]\n",
    ")\n",
    "userdata_df = spark.createDataFrame([], schema)\n",
    "userdata7_df = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for day in range(23, 24):\n",
    "    global userdata7_df\n",
    "    DATA_PREFIX = \"s3a://247machinelearning-curated/kinesis_analytics/{date.year}/{date.month:02d}/{date.day:02d}/*\"\n",
    "    logger.info(\"Reading in data from %d days ago\", day)\n",
    "    START_DATE = datetime.date(2017, 10, 1)\n",
    "    date = START_DATE + datetime.timedelta(days=day - 1)\n",
    "    raw_text_rdd = sc.textFile(DATA_PREFIX.format(date=date))\n",
    "    raw_text_rdd.coalesce(32)\n",
    "    split_text_rdd = raw_text_rdd.flatMap(split_json_lines)\n",
    "    tmp_df = spark\\\n",
    "        .read\\\n",
    "        .json(split_text_rdd)\n",
    "        userdata_df = userdata_df.union(tmp_df)\n",
    "    tmp_df.unpersist()\n",
    "    raw_text_rdd.unpersist()\n",
    "    split_text_rdd.unpersist()\n",
    "userdata_df.repartition(32).alias('userdata7')\n",
    "userdata_df.cache()\n",
    "logger.info(\"Done reading in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userdata_df.createOrReplaceTempView(\"userdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userKey_counts = spark.sql(\"SELECT userId, COUNT(*) AS user_hits \"\n",
    "    \"FROM userdata GROUP BY userId ORDER BY user_hits DESC\")\n",
    "user_df = userKey_counts.filter( \\\n",
    "    userKey_counts.user_id <> \"null\").select( \\\n",
    "    \"user_id\", \"user_hits\", \\\n",
    " F.row_number().over( \\\n",
    "     Window.orderBy(desc(\"user_hits\"))).alias(\"user_id_int\"))\n",
    "\n",
    "user_df.createOrReplaceTempView(\"user\")\n",
    "\n",
    "contentKey_counts = spark.sql(\"SELECT content_key, COUNT(*) \\\n",
    "    AS content_hits FROM userdata GROUP BY content_key ORDER BY content_hits DESC\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als_df = spark.sql(\"SELECT user_id_int, content_key_int, user_content_hits \\\n",
    "FROM user AS u \\\n",
    "INNER JOIN user_content AS UCC ON u.user_id = UCC.user_id \\ \n",
    "INNER JOIN content AS C on UCC.content_key = C.content_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = als_df.randomSplit([0.8, 0.2], seed = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for fitting the model using the Alternating Least Squares algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als = ALS(maxIter = 10, regParam = 0.01, userCol = \"user_id_int\", itemCol = \"content_key_int\", \n",
    "          ratingCol = \"user_content_hits\", implicitPrefs = True, \n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions for the training data set and the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictions = moodel.transform(training)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName = \"rmse\", labelCol = \"user_content_hits\", predictionCol = \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse_train = evaluator.evaluate(train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RMSE: 8.34113047446   \n",
    "Test RMSE: 8.3101470766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_user = test.filter(test.user_id_int = 8).select(\"*\")\n",
    "ten_users = test.filter(test.user_id_int < 11).select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_user_rec = model.transform(single_user)\n",
    "ten_user_rec = model.transform(ten_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_user_rec.createOrReplaceTempView(\"single_user\")\n",
    "single_user_rec.orderBy(\"prediction\", ascending = False).show()\n",
    "ten_user_rec.createOrReplaceTempView(\"ten_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset data in order to build recommendations for the top 10 users, based on user hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ten_users = test.filter(test.user_id_int < 11).select(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data fed into ALS algorithm for top ten users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`>>> ten_user.show()`\n",
    "\n",
    "\n",
    "|user_id_int|content_key_int|user_content_hits|\n",
    "|-----------|---------------|-----------------|\n",
    "|          6|              8|               96|\n",
    "|          7|              8|               33|\n",
    "|          9|            135|                2|\n",
    "|          8|             31|                2|\n",
    "|          8|            407|                1|\n",
    "|          3|            697|               19|\n",
    "|          9|             94|                1|\n",
    "|          6|              1|               45|\n",
    "|          7|              1|               38|\n",
    "|          9|              3|               28|\n",
    "|          4|             10|                2|\n",
    "|          1|            127|                1|\n",
    "|          8|            172|                2|\n",
    "|          3|            132|                8|\n",
    "|          8|             12|               13|\n",
    "|          8|            132|                1|\n",
    "|          9|            493|                1|\n",
    "|          4|              6|              158|\n",
    "|          7|             36|               36|\n",
    "|          7|            113|               12|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show ALS model prediction output from top 10 users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`>>> ten_user.show()`\n",
    "\n",
    "|user_id_int|content_key_int|user_content_hits|  prediction|\n",
    "|-----------|---------------|-----------------|------------|\n",
    "|          8|             12|               13|   1.4914155|\n",
    "|          9|              3|               28|   0.9609676|\n",
    "|          6|              1|               45|  0.75494903|\n",
    "|          7|              1|               38|   0.6643761|\n",
    "|          8|             31|                2|   0.6480088|\n",
    "|          7|              8|               33|   0.5364247|\n",
    "|          7|             36|               36|  0.50630975|\n",
    "|          7|            113|               12|  0.22422674|\n",
    "|          6|              8|               96|  0.18645664|\n",
    "|          8|            132|                1|  0.07000649|\n",
    "|          1|            127|                1| 0.062984094|\n",
    "|          9|            135|                2| 0.041216828|\n",
    "|          3|            132|                8|  0.03759442|\n",
    "|          8|            172|                2|  0.03156671|\n",
    "|          8|            407|                1| 0.012930483|\n",
    "|          9|            493|                1|0.0017545973|\n",
    "|          3|            697|               19|9.2680776E-4|\n",
    "|          4|             10|                2|-0.024427962|\n",
    "|          9|             94|                1| -0.13382056|\n",
    "|          4|              6|              158| -0.85929865|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_unique = spark.sql(\"SELECT COUNT(DISTINCT content_key) AS contents FROM userdata \")\n",
    "pathroot_unique = spark.sql(\"SELECT COUNT(DISTINCT path_root) AS path_roots FROM userdata \")\n",
    "content_pathroot = userdata_df.select(concat(col(\"content_key\"), lit(\" \"), col(\"path_root\"))).distinct().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
